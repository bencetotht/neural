{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Torch 2.4.1+cu121 with torchvision 0.19.1+cu121 on device cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'Running Torch {torch.__version__} with torchvision {torchvision.__version__} on device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:02<00:00, 11090996.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 1045538.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:01<00:00, 3971142.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 3241110.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = datasets.FashionMNIST('./data', download=True, train=True, transform=ToTensor(), target_transform=None)\n",
    "test_dataset = datasets.FashionMNIST('./data', download=True, train=False, transform=ToTensor(), target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '9')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.squeeze(img), cmap='gray')\n",
    "plt.grid(False)\n",
    "plt.axis(False)\n",
    "plt.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating mini-batches\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class names\n",
    "class_names = train_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "img.shape #[32, 1, 28, 28] batch, color, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelV0(nn.Module):\n",
    "    def __init__(self, in_shape: int, hidden: int, out_shape: int):\n",
    "        super().__init__()\n",
    "        # fist convolutional block\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_shape, out_channels=hidden, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden, out_channels=hidden, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # second convolutional block\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden, out_channels=hidden, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden, out_channels=hidden, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2) # by default stride will equal to kernel size\n",
    "        )\n",
    "        # classifier block\n",
    "        self.c1 = nn.Sequential(\n",
    "            nn.Flatten(), # [32, 10*7*7]\n",
    "            nn.Linear(in_features=hidden*7*7, out_features=out_shape) # 7*7 because of the flatten layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.b1(x) # [32, 10, 14, 14])\n",
    "        x = self.b2(x) # [32, 10, 7, 7]\n",
    "        x = self.c1(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelV0(in_shape=1, hidden=10, out_shape=len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn(size=(32, 3, 64, 64))\n",
    "# images batch shape: [32, 3, 64, 64]\n",
    "# image shape: [3, 64, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params = model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    return (correct / len(y_pred)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3480 | Train accuracy: 87.4733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:21<00:43, 21.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3486 | Test accuracy: 87.5899\n",
      "Train loss: 0.3053 | Train accuracy: 88.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:44<00:22, 22.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3131 | Test accuracy: 88.7879\n",
      "Train loss: 0.2858 | Train accuracy: 89.7750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:06<00:00, 22.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3091 | Test accuracy: 89.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # iterating through batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = criterion(y_pred, y) # prediciton, target\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy(y_pred.argmax(dim=1), y) # argmax: logits -> pred labels\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    print(f'Train loss: {train_loss:.4f} | Train accuracy: {train_acc:.4f}')\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            test_pred = model(X)\n",
    "\n",
    "            test_loss += criterion(test_pred, y)\n",
    "            test_acc += accuracy(test_pred.argmax(dim=1), y)\n",
    "\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "        print(f'Test loss: {test_loss:.4f} | Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
